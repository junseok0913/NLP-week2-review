{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# score_submission.ipynb\n",
        "# 제출 파일(submission)과 정답(answer)을 비교해 character-level micro F1을 계산합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def num_same_chars(prediction: str, answer: str) -> int:\n",
        "    same = 0\n",
        "    for i in range(min(len(prediction), len(answer))):\n",
        "        if prediction[i] == answer[i]:\n",
        "            same += 1\n",
        "    return same\n",
        "\n",
        "\n",
        "def f1_from_counts(num_same: int, pred_len: int, answer_len: int) -> float:\n",
        "    if pred_len == 0 and answer_len == 0:\n",
        "        return 1.0\n",
        "    if pred_len == 0 or answer_len == 0:\n",
        "        return 0.0\n",
        "    precision = num_same / pred_len\n",
        "    recall = num_same / answer_len\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "    return 2 * precision * recall / (precision + recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "submission_path에서 제출 파일명을 설정해주세요!\n",
        "'''\n",
        "submission_path = #Path('submission/submission.csv')\n",
        "answer_path = Path('data/answer.csv')\n",
        "\n",
        "id_col = 'ID'\n",
        "pred_col = 'output'\n",
        "answer_col = 'output'\n",
        "encoding = 'utf-8-sig'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def score_submission(\n",
        "    submission_path: Path | str,\n",
        "    answer_path: Path | str,\n",
        "    *,\n",
        "    id_col: str = 'ID',\n",
        "    pred_col: str = 'output',\n",
        "    answer_col: str = 'output',\n",
        "    encoding: str = 'utf-8-sig',\n",
        ") -> tuple[int, float]:\n",
        "    submission_path = Path(submission_path)\n",
        "    answer_path = Path(answer_path)\n",
        "\n",
        "    sub_df = pd.read_csv(submission_path, encoding=encoding)\n",
        "    ans_df = pd.read_csv(answer_path, encoding=encoding)\n",
        "\n",
        "    for path, df in [(submission_path, sub_df), (answer_path, ans_df)]:\n",
        "        if id_col not in df.columns:\n",
        "            raise ValueError(\n",
        "                f'Missing column {id_col!r} in {path}. Columns: {df.columns.tolist()}'\n",
        "            )\n",
        "\n",
        "    if pred_col not in sub_df.columns:\n",
        "        raise ValueError(\n",
        "            f'Missing prediction column {pred_col!r} in {submission_path}. '\n",
        "            f'Columns: {sub_df.columns.tolist()}'\n",
        "        )\n",
        "    if answer_col not in ans_df.columns:\n",
        "        raise ValueError(\n",
        "            f'Missing answer column {answer_col!r} in {answer_path}. '\n",
        "            f'Columns: {ans_df.columns.tolist()}'\n",
        "        )\n",
        "\n",
        "    sub_df = sub_df[[id_col, pred_col]].copy()\n",
        "    ans_df = ans_df[[id_col, answer_col]].copy()\n",
        "\n",
        "    if sub_df[id_col].isna().any():\n",
        "        raise ValueError(f'{submission_path} contains empty IDs.')\n",
        "    if ans_df[id_col].isna().any():\n",
        "        raise ValueError(f'{answer_path} contains empty IDs.')\n",
        "\n",
        "    if sub_df[id_col].duplicated().any():\n",
        "        dup_examples = (\n",
        "            sub_df.loc[sub_df[id_col].duplicated(), id_col].astype(str).head(10).tolist()\n",
        "        )\n",
        "        raise ValueError(f'Duplicate IDs found in {submission_path}. Examples: {dup_examples}')\n",
        "    if ans_df[id_col].duplicated().any():\n",
        "        dup_examples = (\n",
        "            ans_df.loc[ans_df[id_col].duplicated(), id_col].astype(str).head(10).tolist()\n",
        "        )\n",
        "        raise ValueError(f'Duplicate IDs found in {answer_path}. Examples: {dup_examples}')\n",
        "\n",
        "    sub_df[id_col] = sub_df[id_col].astype(str)\n",
        "    ans_df[id_col] = ans_df[id_col].astype(str)\n",
        "\n",
        "    sub_ids = set(sub_df[id_col])\n",
        "    ans_ids = set(ans_df[id_col])\n",
        "    missing = ans_ids - sub_ids\n",
        "    extra = sub_ids - ans_ids\n",
        "    if missing or extra:\n",
        "        parts: list[str] = ['ID mismatch between submission and answer.']\n",
        "        if missing:\n",
        "            preview = sorted(list(missing))[:10]\n",
        "            parts.append(f'- Missing in submission: {len(missing)} (e.g. {preview})')\n",
        "        if extra:\n",
        "            preview = sorted(list(extra))[:10]\n",
        "            parts.append(f'- Extra in submission: {len(extra)} (e.g. {preview})')\n",
        "        raise ValueError('\\n'.join(parts))\n",
        "\n",
        "    sub_df = sub_df.rename(columns={pred_col: 'prediction'})\n",
        "    ans_df = ans_df.rename(columns={answer_col: 'answer'})\n",
        "\n",
        "    merged = ans_df.merge(sub_df, on=id_col, how='inner', validate='one_to_one')\n",
        "    if len(merged) != len(ans_df):\n",
        "        raise ValueError(\n",
        "            'Internal error: merged rows != answer rows '\n",
        "            f'({len(merged)} != {len(ans_df)}).'\n",
        "        )\n",
        "\n",
        "    merged['prediction'] = merged['prediction'].fillna('').astype(str)\n",
        "    merged['answer'] = merged['answer'].fillna('').astype(str)\n",
        "\n",
        "    total_same = 0\n",
        "    total_pred_len = 0\n",
        "    total_answer_len = 0\n",
        "\n",
        "    for prediction, answer in zip(merged['prediction'].tolist(), merged['answer'].tolist()):\n",
        "        pred_len = len(prediction)\n",
        "        answer_len = len(answer)\n",
        "        num_same = num_same_chars(prediction, answer)\n",
        "\n",
        "        total_same += num_same\n",
        "        total_pred_len += pred_len\n",
        "        total_answer_len += answer_len\n",
        "\n",
        "    f1 = f1_from_counts(total_same, total_pred_len, total_answer_len)\n",
        "    return len(merged), f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows: 1263\n",
            "F1: 0.971315\n"
          ]
        }
      ],
      "source": [
        "rows, f1 = score_submission(\n",
        "    submission_path,\n",
        "    answer_path,\n",
        "    id_col=id_col,\n",
        "    pred_col=pred_col,\n",
        "    answer_col=answer_col,\n",
        "    encoding=encoding,\n",
        ")\n",
        "\n",
        "print(f'Rows: {rows}')\n",
        "print(f'F1: {f1:.6f}')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py311-base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
