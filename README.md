# NLP-week2-review

KUBIG NLP 2주차 복습과제입니다.

---

## 0. 과제 안내

### 목표

> **더 적은 학습 및 추론 시간**으로 **베이스라인 성능(F1:0.965963)을 넘기세요!**

단순히 성능만 높이는 것이 아니라, **효율성**도 함께 고려해야 합니다.

### 제출 파일

| 파일 | 설명 |
|------|------|
| `*.ipynb` | 학습 및 추론 코드 |
| `*.csv` | 예측 결과 파일 |
| `*.pth` | 학습된 모델 가중치 |

**제출 위치**: `KUBIG_2026_SPRING/Basic_Study/NLP/week2/복습과제1/`

**예시**
```
/NLP/week2/복습과제1/
├── 강준석.ipynb
├── 강준석.csv
├── 강준석_cbow.pth
└── 강준석_bi-gru-4.pth
```

### 시도해볼 만한 것들

#### 모델 구조 변경
| 시도 | 설명 |
|------|------|
| **BiLSTM** | GRU 대신 LSTM 사용 (forget gate로 장기 의존성 개선) |
| **Dropout** | 과적합 방지 및 일반화 성능 향상 |
| **Layer Normalization** | 학습 안정화 및 수렴 속도 개선 |
| **Residual Connection** | 깊은 네트워크에서 그래디언트 흐름 개선 |
| **Attention 추가** | 중요한 문맥에 집중 |

#### 임베딩 개선
| 시도 | 설명 |
|------|------|
| **Skip-gram** | CBOW 대신 Skip-gram 사용 (희귀 문자에 강점) |
| **Window size 조정** | 문맥 범위 변경 (작으면 지역적, 크면 전역적) |
| **Embedding 차원 조정** | 차원 축소로 효율성 개선 or 확대로 표현력 증가 |
| **자모 단위 임베딩** | 한글을 초성/중성/종성으로 분해하여 학습 |
| **Embedding fine-tune** | freeze 해제하고 메인 모델과 함께 학습 |
| **사전학습 임베딩** | 외부 한글 임베딩 활용 |

---

## 1. 대회 내용

### Task: 난독화된 한글 리뷰 복원

해외 숙소 예약 사이트에서 부정적인 리뷰가 삭제될 수 있어, 한국인들끼리만 알아볼 수 있도록 **한글을 난독화**하는 방식이 등장했습니다. 이 과제는 난독화된 리뷰를 원래의 정상적인 한글로 복원하는 AI 모델을 개발하는 것이 목표입니다.

### 예시

| 난독화 입력 | 정상 출력 |
|-------------|-----------|
| `온션퓨 갸셩피 쑥쏘웩오!` | `오션뷰 가성비 숙소에요!` |
| `힐륑햐려 왔눈뎨` | `힐링하러 왔는데` |
| `쥑건뿐둘또 췬졀햐쎄오` | `직원분들도 친절하세요` |
| `깔큼햐교` | `깔끔하고` |

### 난독화 특징
- 초성/중성/종성을 유사한 발음의 다른 자모로 치환
- 모음 변형 (예: `ㅏ` → `ㅑ`, `ㅓ` → `ㅕ`)
- 자음 변형 (예: `ㄱ` → `ㅋ`, `ㄷ` → `ㅌ`)
- 받침 변형

### 데이터셋

| 파일 | 설명 |
|------|------|
| `data/train.csv` | 학습 데이터 (input, output 쌍) |
| `data/test.csv` | 테스트 데이터 (input만 제공) |
| `data/sample_submission.csv` | 제출 양식 |
| `data/answer.csv` | 정답 파일 (**평가용, 코드에서 사용 금지!**) |

> **주의**: `answer.csv`를 코드에서 사용하면 **Data Leakage**입니다!

### 평가 메트릭: Character-level Micro F1

```
Precision = 일치 문자 수 / 예측 문자열 길이
Recall    = 일치 문자 수 / 정답 문자열 길이
F1        = 2 × Precision × Recall / (Precision + Recall)
```

**계산 예시 1: 완벽 일치**
```
예측: "안녕하세요"
정답: "안녕하세요"

일치: 5개 / 예측 길이: 5 / 정답 길이: 5
F1 = 1.0
```

**계산 예시 2: 부분 일치**
```
예측: "안녕하세요"
정답: "안녕히세용"

위치별 비교: 안(O) 녕(O) 하/히(X) 세(O) 요/용(X)
일치: 3개 / 예측 길이: 5 / 정답 길이: 5

Precision = 3/5 = 0.6
Recall    = 3/5 = 0.6
F1 = 0.6
```

**계산 예시 3: 위치 밀림 (주의!)**
```
예측: "직원분들 친절해요"  ("도" 누락)
정답: "직원분들도 친절해요"

위치별 비교:
  직(O) 원(O) 분(O) 들(O) [공백/도](X) [친/공백](X) [절/친](X) ...

→ 한 글자 누락으로 뒤쪽 전체가 밀려서 오답 처리!
→ F1 급락
```

> **핵심**: 이 평가 방식은 **위치 기반**이므로, 한 글자라도 추가/삭제되면 이후 모든 문자가 틀린 것으로 처리됩니다.

---

## 2. Baseline 설명

### 왜 Token Classification인가?

평가 방식이 **위치별 문자 비교**이므로, Seq2Seq 생성 모델은 길이 변동 위험이 있어 불리합니다.

```
┌─────────────────────────────────────────────────────────┐
│              Seq2Seq (위험)                              │
├─────────────────────────────────────────────────────────┤
│  입력: [갸, 셩, 피]                                       │
│           ↓                                             │
│       Encoder → Decoder (자유롭게 생성)                   │
│           ↓                                             │
│  출력: [가, 성, 비, 야] ← 길이가 달라질 수 있음!             │
└─────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────┐
│              Token Classification (안전)                 │
├─────────────────────────────────────────────────────────┤
│  입력: [갸, 셩, 피]                                       │
│         ↓   ↓   ↓   (각 위치별 독립 분류)                  │
│  출력: [가, 성, 비]  ← 길이가 항상 동일!                    │
└─────────────────────────────────────────────────────────┘
```

### 모델 구조

```
입력 → Embedding → Bidirectional GRU (4층) → Linear → 출력
       (CBOW 사전학습)
```

| 구성 요소 | 설명 |
|-----------|------|
| **Tokenizer** | Character-level (문자 단위) |
| **Embedding** | CBOW로 사전 학습 (512차원) |
| **Encoder** | Bidirectional Multi-layer GRU (4층, hidden=512) |
| **Output** | 각 위치에서 vocab_size 크기로 분류 |

### 설계 의도

| 설계 요소 | 이유 |
|-----------|------|
| **Decoder 없음** | 길이 변동 방지 |
| **GRU만 사용** | 입력 길이 T → 출력 길이 T 유지 |
| **각 위치별 Linear** | 위치 i의 입력 → 위치 i의 출력 (1:1 매핑) |
| **Bidirectional** | 앞뒤 문맥 모두 활용하여 정확도 향상 |
| **CBOW 사전학습** | 문자 임베딩의 품질 향상 |

### Baseline 성능

| 모델 | F1 Score |
|------|----------|
| Baseline (BiGRU) | 0.965963 |

---

## 3. 평가 방법 (evaluate.ipynb)

### 사용법

1. `submission/` 폴더에 예측 결과 CSV 파일 저장

2. `evaluate.ipynb`에서 `submission_path` 설정:
```python
submission_path = Path('submission/your_submission.csv')  # 본인 파일명으로 수정
answer_path = Path('data/answer.csv')
```

3. 노트북 실행 → F1 점수 확인

### 예측 결과 파일 형식

```csv
ID,output
ID_10877,복원된 리뷰 텍스트
ID_02074,복원된 리뷰 텍스트
...
```

### 주의사항
- `answer.csv`는 **오직 평가용**으로만 사용
- 코드에서 `answer.csv`를 참조하면 **Data Leakage**!
